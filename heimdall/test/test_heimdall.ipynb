{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect.session import DatabricksSession as SparkSession\n",
    "from databricks.sdk.core import Config\n",
    "from pyspark.testing.utils import assertDataFrameEqual\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from heimdall import StorageContext\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(profile='DEFAULT', cluster_id='1017-032629-beha6p1')\n",
    "spark = SparkSession.builder.sdkConfig(config).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestStorageContext(unittest.TestCase):\n",
    " \n",
    "    def test_initialization(self):\n",
    "\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"params\": {\n",
    "                    \"storage_account\": \"stpandoraprdheimdall\",\n",
    "                    \"path\": \"/blobServices/default/containers/risco-abertura-infracao-pix/blobs/\",\n",
    "                    \"blob\": \"2023_11_16_17_57_24_3rd_willian_cavalcante.csv\",\n",
    "                    \"invocation_id\": \"f93199c3-dd33-4aad-ae8c-c8eefb0164c6\"\n",
    "                },\n",
    "                \"expected\": {\n",
    "                    \"storage_account\": \"stpandoraprdheimdall\",\n",
    "                    \"invocation_id\": \"f93199c3-dd33-4aad-ae8c-c8eefb0164c6\",\n",
    "                    \"container\": \"risco-abertura-infracao-pix\",\n",
    "                    \"blob\": \"2023_11_16_17_57_24_3rd_willian_cavalcante.csv\"\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"params\": {\n",
    "                    \"storage_account\": \"stpandoraxpto\",\n",
    "                    \"path\": \"/blobServices/default/containers/xpto/blobs/path1/paht2\",\n",
    "                    \"blob\": \"2023_11_16_17_57_24_3rd_willian_cavalcante.csv\",\n",
    "                    \"invocation_id\": \"123-abc\"\n",
    "                },\n",
    "                \"expected\": {\n",
    "                    \"storage_account\": \"stpandoraxpto\",\n",
    "                    \"invocation_id\": \"123-abc\",\n",
    "                    \"container\": \"xpto\",\n",
    "                    \"blob\": \"2023_11_16_17_57_24_3rd_willian_cavalcante.csv\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        for case in test_cases:\n",
    "            with self.subTest(case=case):\n",
    "                storage_context = StorageContext(**case[\"params\"])\n",
    "                for attr, expected_value in case[\"expected\"].items():\n",
    "                    actual_value = getattr(storage_context, attr)\n",
    "                    self.assertEqual(actual_value, expected_value)\n",
    "    \n",
    "    def test_extract_value(self):\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"params\": {\n",
    "                    \"pattern\": r\"containers/([^/]+)\",\n",
    "                    \"text\": \"/blobServices/default/containers/risco-abertura-infracao-pix/blobs/\"\n",
    "                },\n",
    "                \"expected\": \"risco-abertura-infracao-pix\"\n",
    "            },\n",
    "            {\n",
    "                \"params\": {\n",
    "                    \"pattern\": r\"blobs/(.+)\",\n",
    "                    \"text\": \"/blobServices/default/containers/risco-abertura-infracao-pix/blobs/myfile.csv\"\n",
    "                },\n",
    "                \"expected\": \"myfile.csv\"\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        for case in test_cases:\n",
    "            with self.subTest(case=case):\n",
    "                result = StorageContext._extract_value(**case[\"params\"])\n",
    "                self.assertEqual(result, case[\"expected\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dataframe_from_table():\n",
    "        # Lendo o DataFrame da tabela (ajuste este comando para sua fonte de dados)\n",
    "        df = spark.read.table(\"hive_metastore.default.departments\")\n",
    "\n",
    "        # Dados esperados\n",
    "        expected_data = [(1, \"Administration\"), (2, \"Human Resource\"), (3, \"Engineering\")]\n",
    "        # Schema para o DataFrame\n",
    "        schema = \"id INT, name STRING\"\n",
    "\n",
    "        # Criar DataFrame esperado\n",
    "        expected_df = spark.createDataFrame(expected_data, schema)\n",
    "\n",
    "        # Assert\n",
    "        assertDataFrameEqual(df, expected_df)\n",
    "\n",
    "def test_dataframe_equality():\n",
    "        data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
    "        df1 = spark.createDataFrame(data, [\"name\", \"id\"])\n",
    "        df2 = spark.createDataFrame(data, [\"name\", \"id\"])\n",
    "\n",
    "        assertDataFrameEqual(df1, df2)\n",
    "\n",
    "def test_dataframe_filtering():\n",
    "    data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "    df = spark.createDataFrame(data, [\"name\", \"id\"])\n",
    "    filtered_df = df.filter(df[\"id\"] > 1)\n",
    "\n",
    "    expected_data = [(\"Bob\", 2), (\"Charlie\", 3)]\n",
    "    expected_df = spark.createDataFrame(expected_data, [\"name\", \"id\"])\n",
    "\n",
    "    assertDataFrameEqual(filtered_df, expected_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
